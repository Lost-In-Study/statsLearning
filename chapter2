1.
(a) better
(b) worse
(c) better
(dï¼‰worse

2.
(a) regression, inference
(b) classfication, prediction
(c) regression, prediction

3.
(a)
(b) all 5 lines >= 0

i. (squared) bias - decreases monotonically because increases in flexibility
yield a closer fit

ii. variance - increases monotonically because increases in flexibility yield
overfit

iii. training error - decreases monotonically because increases in flexibility
yield a closer fit

iv. test error - concave up curve because increase in flexibility yields a closer
fit before it overfits

v. Bayes (irreducible) error - defines the lower limit, the test error is bounded 
below by the irreducible error due to variance in the error (epsilon) in the output 
values (0 <= value). When the training error is lower than the irreducible error,
overfitting has taken place.
The Bayes error rate is defined for classification problems and is determined by 
the ratio of data points which lie at the 'wrong' side of the decision boundary, 
(0 <= value < 1).

4. 
(a) detect spam email: 
             response: spam or not
             predictor: title, subject, link, content...
             goal: prediction
    recognize manual digit:
             response: 1,2,3,4,5,6,7,8,9
             predictor: pixel position
             goal: prediction
    recognize sharks using picture:
             response: shark or not
             predictor: picture features
             goal: prediction
(b) predict stock price:
        
5. advantage: low training error, more acurate, low bias
    disadvantage: overfitting, high variance
    when more interested in prediction not in inference or verse
6. parametric vs non-parametric
7. 
(a) 
(b)
(c)
(d) small, small K is more flexible, large K will try to fit to a more linear boundary.
    
